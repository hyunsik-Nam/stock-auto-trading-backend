    
import os
from typing import Any, Dict, List

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from ..utils.promptManager import YAMLPromptManager
from langchain_core.runnables import RunnableBranch, RunnablePassthrough
from langchain.output_parsers.json import SimpleJsonOutputParser
from ..utils.structured_outputs import StockStruct,FinalStockStruct,OrderClassifier
from ..utils.llm_tools import *
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from langchain_core.outputs import LLMResult
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
import json


load_dotenv()

google_api_key = os.getenv("GOOGLE_API_KEY")

os.environ["GOOGLE_API_KEY"] = google_api_key or ""

class LoggingHandler(BaseCallbackHandler):
    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs
    ) -> None:
        print("Chat model started")

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        print(f"Chat model ended, response: {response}")

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs
    ) -> None:
        print(f"Chain {serialized.get('name')} started")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        print(f"Chain ended, outputs: {outputs}")

callbacks = [LoggingHandler()]
model = init_chat_model("gemini-2.5-flash", model_provider="google_genai")
json_parser = SimpleJsonOutputParser()
structured_llm = model.with_structured_output(FinalStockStruct)
yaml_prompt_manager = YAMLPromptManager()



class LLMService:
    def __init__(self):
        pass
    
    def _create_routing_chain(self):
        """ë¼ìš°íŒ… ì²´ì¸ ìƒì„±"""
        
        # ê° ì–´ë“œë°”ì´ì € í”„ë¡¬í”„íŠ¸
        def stock_prompt(question: str):
            context = 'testì…ë‹ˆë‹¤'
            prompt = yaml_prompt_manager.create_chat_prompt("stock_advisor", context=context, question=question)
            return prompt


        # general_prompt = yaml_prompt_manager.create_chat_prompt("general_advisor")

        def general_prompt(question: str) :
            context = 'testì…ë‹ˆë‹¤'
            prompt = yaml_prompt_manager.create_chat_prompt("general_advisor", context=context, question=question)
            return prompt

        def extract_content(chunk):
            print("extract_content:", chunk)
            return chunk
        
        # ë¶„ë¥˜ê¸°
        classifier =  yaml_prompt_manager.create_chat_prompt("stock_general_branch_prompt") | model
        stock_classifier = yaml_prompt_manager.create_chat_prompt("stock_order_branch") | model.with_structured_output(OrderClassifier)
        

        # ğŸ¯ RunnableBranchë¡œ ë¶„ê¸°ì²˜ë¦¬
        # routing_chain = RunnableBranch(
        #     (
        #         # 1ì°¨ ë¶„ê¸°: STOCK ì—¬ë¶€
        #         lambda x: "STOCK" in classifier.invoke({"question": x["question"]}).content.upper(),
        #         lambda x: RunnableBranch(
        #             (
        #                 # 2ì°¨ ë¶„ê¸°: STOCK_ORDER ì—¬ë¶€
        #                 lambda x: "STOCK_ORDER" == stock_classifier.invoke({"question": x["question"]}).content.get("type", ""),
        #                 lambda x: RunnableLambda(extract_content) | parse_stock_info | structured_llm | order_stock
        #             ),
        #             (
        #                 # STOCK_GENERALì¼ ê²½ìš°
        #                 lambda x: "STOCK_GENERAL" == stock_classifier.invoke({"question": x["question"]}).content.get("type", ""),
        #                 lambda x: stock_prompt(x["question"]) | model | json_parser
        #             ),
        #         )
        #     ),
        #     (
        #         # ê¸°ë³¸ê°’(GENERAL)
        #         lambda x: True,
        #         lambda x: general_prompt(x["question"]) | model | json_parser
        #     )
        # )

        def wrap_stock_data(data):
            return {"stock_data": data}

        # 1ì°¨ ë¶„ê¸° ì •ì˜
        routing_chain = RunnableBranch(
            (
                # STOCK ì—¬ë¶€ ì²´í¬
                lambda x: "STOCK" in classifier.invoke({"question": x["question"]}).content.upper(),
                RunnableBranch(
                    (
                        # STOCK_ORDER ì²´í¬
                        lambda x: "STOCK_ORDER" == stock_classifier.invoke({"question": x["question"]}).get("type").upper(),
                        parse_stock_info | structured_llm | order_stock
                    ),
                    # ê¸°ë³¸ê°’ (ë‹¤ë¥¸ STOCK ê´€ë ¨)
                    lambda x: stock_prompt(x["question"]) | model | json_parser
                )
            ),
            lambda x: general_prompt(x["question"]) | model | json_parser
        )
        
        return routing_chain
    

    async def advisor_stream(self, question):
        """ìƒë‹´ ìŠ¤íŠ¸ë¦¼"""
        try:
            # ë¨¼ì € ë¶„ë¥˜ ë©”ì‹œì§€ ì „ì†¡
            classification_data = {"content": f"ìƒë‹´ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n\n"}
            yield f"data: {json.dumps(classification_data, ensure_ascii=False)}\n\n"

            # response = await self._create_routing_chain().invoke({"question": question}, config={"callbacks": callbacks})

            # Chainìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            async for chunk in self._create_routing_chain().astream({"question": question}, config={"callbacks": callbacks}):
                # contentê°€ ìˆìœ¼ë©´ í•œ ë²ˆì— ì²˜ë¦¬
                content = chunk.get("content", "") if isinstance(chunk, dict) else ""
                # for char in content:
                #     yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
                    # contentê°€ dictë©´ json ë¬¸ìì—´ë¡œ ë³€í™˜
                if isinstance(content, dict):
                    content_str = json.dumps(content, ensure_ascii=False)
                    yield f"data: {json.dumps({'content': content_str}, ensure_ascii=False)}\n\n"
                else:
                    for char in content:
                        yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            error_data = {"content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
    
