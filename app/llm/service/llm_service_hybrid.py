import os
from typing import Any, Dict, List, TypedDict, Literal
import json

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from langchain_core.outputs import LLMResult
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.output_parsers.json import SimpleJsonOutputParser

# LangGraph imports
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages

# Your existing imports
from ..utils.promptManager import YAMLPromptManager
from ..utils.structured_outputs import StockStruct, FinalStockStruct, OrderClassifier
from ..utils.llm_tools import *


load_dotenv()

google_api_key = os.getenv("GOOGLE_API_KEY")

os.environ["GOOGLE_API_KEY"] = google_api_key or ""

class LoggingHandler(BaseCallbackHandler):
    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs
    ) -> None:
        print("Chat model started")

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        print(f"Chat model ended, response: {response}")

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs
    ) -> None:
        print(f"Chain {serialized.get('name')} started")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        print(f"Chain ended, outputs: {outputs}")


# State ì •ì˜ - ë” êµ¬ì¡°í™”ëœ ìƒíƒœ
class AdvisorState(TypedDict):
    question: str
    main_classification: dict
    stock_classification: dict
    route: str
    processed_data: dict  # ì¤‘ê°„ ì²˜ë¦¬ ë°ì´í„°
    final_result: Any
    error: str
    metadata: dict  # ì¶”ê°€ ë©”íƒ€ë°ì´í„°


callbacks = [LoggingHandler()]
model = init_chat_model("gemini-2.5-flash", model_provider="google_genai")
json_parser = SimpleJsonOutputParser()
str_parser = StrOutputParser()
structured_llm = model.with_structured_output(FinalStockStruct)
yaml_prompt_manager = YAMLPromptManager()


class HybridLLMService:
    def __init__(self):
        # LCEL ì²´ì¸ë“¤ì„ ë¯¸ë¦¬ êµ¬ì„±
        self._setup_lcel_chains()
    
    def _setup_lcel_chains(self):
        """LCEL ì²´ì¸ë“¤ì„ ë¯¸ë¦¬ êµ¬ì„±"""
        
        # 1ì°¨ ë¶„ë¥˜ ì²´ì¸
        self.main_classifier_chain = (
            yaml_prompt_manager.create_chat_prompt("stock_general_branch_prompt")
            | model
            | RunnableLambda(self._extract_main_classification)
        )
        
        # 2ì°¨ ë¶„ë¥˜ ì²´ì¸ (ì£¼ì‹ ì „ìš©)
        self.stock_classifier_chain = (
            yaml_prompt_manager.create_chat_prompt("stock_order_branch")
            | model.with_structured_output(OrderClassifier)
            | RunnableLambda(self._extract_stock_type)
        )
        
        # ì£¼ì‹ ì¼ë°˜ ìƒë‹´ ì²´ì¸
        self.stock_general_chain = (
            RunnableLambda(self._create_stock_prompt)
            | model
            | json_parser
            | RunnableLambda(self._format_stock_response)
        )
        
        # ì¼ë°˜ ìƒë‹´ ì²´ì¸
        self.general_advice_chain = (
            RunnableLambda(self._create_general_prompt)
            | model
            | json_parser
            | RunnableLambda(self._format_general_response)
        )
        
        # ì£¼ì‹ ì£¼ë¬¸ ì²˜ë¦¬ ì²´ì¸
        self.stock_order_chain = (
            RunnableLambda(parse_stock_info)
            | structured_llm
            | order_stock
            | RunnableLambda(self._format_order_response)
        )
    
    # LCEL ì²´ì¸ì—ì„œ ì‚¬ìš©í•  í—¬í¼ í•¨ìˆ˜ë“¤
    def _extract_main_classification(self, llm_result):
        """1ì°¨ ë¶„ë¥˜ ê²°ê³¼ ì¶”ì¶œ"""
        content = llm_result.content if hasattr(llm_result, 'content') else str(llm_result)
        is_stock = "STOCK" in content.upper()
        return {
            "content": content,
            "is_stock": is_stock,
            "confidence": self._calculate_confidence(content)
        }
    
    def _extract_stock_type(self, structured_result):
        """2ì°¨ ë¶„ë¥˜ ê²°ê³¼ ì¶”ì¶œ"""
        if hasattr(structured_result, 'dict'):
            return structured_result.dict()
        elif isinstance(structured_result, dict):
            return structured_result
        else:
            return {"type": "STOCK_GENERAL", "confidence": 0.5}
    
    def _create_stock_prompt(self, data):
        """ì£¼ì‹ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        question = data.get("question", "")
        context = data.get("context", "testì…ë‹ˆë‹¤")
        return yaml_prompt_manager.create_chat_prompt(
            "stock_advisor", 
            context=context, 
            question=question
        )
    
    def _create_general_prompt(self, data):
        """ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        question = data.get("question", "")
        context = data.get("context", "testì…ë‹ˆë‹¤")
        return yaml_prompt_manager.create_chat_prompt(
            "general_advisor",
            context=context,
            question=question
        )
    
    def _format_stock_response(self, response):
        """ì£¼ì‹ ì‘ë‹µ í¬ë§·íŒ…"""
        return {
            "content": response,
            "type": "stock_advice",
            "category": "investment"
        }
    
    def _format_general_response(self, response):
        """ì¼ë°˜ ì‘ë‹µ í¬ë§·íŒ…"""
        return {
            "content": response,
            "type": "general_advice",
            "category": "general"
        }
    
    def _format_order_response(self, response):
        """ì£¼ë¬¸ ì‘ë‹µ í¬ë§·íŒ…"""
        return {
            "content": response,
            "type": "order_confirmation",
            "category": "transaction"
        }
    
    def _calculate_confidence(self, content):
        """ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        stock_keywords = ["ì£¼ì‹", "íˆ¬ì", "ë§¤ìˆ˜", "ë§¤ë„", "ì¢…ëª©", "ì£¼ê°€"]
        keyword_count = sum(1 for keyword in stock_keywords if keyword in content)
        return min(keyword_count * 0.2 + 0.1, 1.0)
    
    def _create_langgraph_chain(self):
        """LangGraph + LCEL í•˜ì´ë¸Œë¦¬ë“œ ì²´ì¸ ìƒì„±"""
        
        # ë…¸ë“œ í•¨ìˆ˜ë“¤ - LCEL ì²´ì¸ì„ í™œìš©
        def classify_main_node(state: AdvisorState) -> AdvisorState:
            """1ì°¨ ë¶„ë¥˜ ë…¸ë“œ - LCEL ì²´ì¸ ì‚¬ìš©"""
            try:
                # LCEL ì²´ì¸ìœ¼ë¡œ ë¶„ë¥˜ ìˆ˜í–‰
                main_result = self.main_classifier_chain.invoke({
                    "question": state["question"]
                })
                
                route = "STOCK" if main_result["is_stock"] else "GENERAL"
                
                return {
                    **state,
                    "main_classification": main_result,
                    "route": route,
                    "metadata": {"step": "main_classification", "confidence": main_result.get("confidence", 0)}
                }
            except Exception as e:
                return {**state, "error": str(e), "route": "ERROR"}

        def classify_stock_node(state: AdvisorState) -> AdvisorState:
            """2ì°¨ ë¶„ë¥˜ ë…¸ë“œ - LCEL ì²´ì¸ ì‚¬ìš©"""
            try:
                # LCEL ì²´ì¸ìœ¼ë¡œ ì£¼ì‹ ë¶„ë¥˜ ìˆ˜í–‰
                stock_result = self.stock_classifier_chain.invoke({
                    "question": state["question"]
                })
                
                stock_type = stock_result.get("type", "").upper()
                
                return {
                    **state,
                    "stock_classification": stock_result,
                    "route": stock_type,
                    "metadata": {**state.get("metadata", {}), "stock_type": stock_type}
                }
            except Exception as e:
                return {**state, "error": str(e), "route": "ERROR"}

        def process_stock_order_node(state: AdvisorState) -> AdvisorState:
            """ì£¼ì‹ ì£¼ë¬¸ ì²˜ë¦¬ ë…¸ë“œ - LCEL ì²´ì¸ ì‚¬ìš©"""
            try:
                # LCEL ì²´ì¸ìœ¼ë¡œ ì£¼ë¬¸ ì²˜ë¦¬
                result = self.stock_order_chain.invoke(state["stock_classification"])
                
                return {
                    **state, 
                    "final_result": result,
                    "processed_data": {"order_processed": True}
                }
            except Exception as e:
                return {**state, "error": str(e)}

        def process_stock_general_node(state: AdvisorState) -> AdvisorState:
            """ì£¼ì‹ ì¼ë°˜ ìƒë‹´ ë…¸ë“œ - LCEL ì²´ì¸ ì‚¬ìš©"""
            try:
                # LCEL ì²´ì¸ìœ¼ë¡œ ì£¼ì‹ ìƒë‹´ ì²˜ë¦¬
                result = self.stock_general_chain.invoke({
                    "question": state["question"],
                    "context": "ì£¼ì‹ ì „ë¬¸ ìƒë‹´"
                })
                
                return {
                    **state,
                    "final_result": result,
                    "processed_data": {"advice_type": "stock_general"}
                }
            except Exception as e:
                return {**state, "error": str(e)}

        def process_general_node(state: AdvisorState) -> AdvisorState:
            """ì¼ë°˜ ìƒë‹´ ë…¸ë“œ - LCEL ì²´ì¸ ì‚¬ìš©"""
            try:
                # LCEL ì²´ì¸ìœ¼ë¡œ ì¼ë°˜ ìƒë‹´ ì²˜ë¦¬
                result = self.general_advice_chain.invoke({
                    "question": state["question"],
                    "context": "ì¼ë°˜ ìƒë‹´"
                })
                
                return {
                    **state,
                    "final_result": result,
                    "processed_data": {"advice_type": "general"}
                }
            except Exception as e:
                return {**state, "error": str(e)}

        def handle_error_node(state: AdvisorState) -> AdvisorState:
            """ì—ëŸ¬ ì²˜ë¦¬ ë…¸ë“œ - êµ¬ì¡°í™”ëœ ì—ëŸ¬ ì‘ë‹µ"""
            error_chain = (
                RunnableLambda(lambda x: {
                    "error": x.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"),
                    "step": x.get("metadata", {}).get("step", "unknown"),
                    "question": x.get("question", "")
                })
                | RunnableLambda(lambda x: {
                    "content": f"ì£„ì†¡í•©ë‹ˆë‹¤. {x['step']} ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {x['error']}",
                    "type": "error",
                    "recovery_suggestion": "ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œê±°ë‚˜ ë¬¸ì˜ì‚¬í•­ì„ ë‚¨ê²¨ì£¼ì„¸ìš”."
                })
            )
            
            error_result = error_chain.invoke(state)
            return {**state, "final_result": error_result}

        # ë¼ìš°íŒ… í•¨ìˆ˜ë“¤ - ë” ì •êµí•œ ì¡°ê±´ ì²˜ë¦¬
        def route_after_main_classification(state: AdvisorState) -> Literal["classify_stock", "process_general", "handle_error"]:
            """ë©”ì¸ ë¶„ë¥˜ í›„ ë¼ìš°íŒ… - ê°œì„ ëœ ì¡°ê±´ë¶€ ë¡œì§"""
            route = state.get("route", "")
            main_classification = state.get("main_classification", {})
            
            # ì—ëŸ¬ ì²˜ë¦¬
            if route == "ERROR":
                return "handle_error"
            elif route == "STOCK":
                # ì‹ ë¢°ë„ ê¸°ë°˜ ì¶”ê°€ ê²€ì¦
                confidence = main_classification.get("confidence", 0)
                if confidence < 0.3:  # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì¼ë°˜ ì²˜ë¦¬
                    return "process_general"
                return "classify_stock"
            else:
                # í‚¤ì›Œë“œ ê¸°ë°˜ 2ì°¨ ê²€ì¦
                question = state.get("question", "").lower()
                stock_keywords = ["ì‚¼ì„±", "lg", "í˜„ëŒ€", "ì£¼ê°€", "íˆ¬ì", "ë§¤ìˆ˜", "ë§¤ë„"]
                if any(keyword in question for keyword in stock_keywords):
                    return "classify_stock"
                return "process_general"

        def route_after_stock_classification(state: AdvisorState) -> Literal["process_stock_order", "process_stock_general", "handle_error"]:
            """ì£¼ì‹ ë¶„ë¥˜ í›„ ë¼ìš°íŒ…"""
            route = state.get("route", "")
            
            if route == "ERROR":
                return "handle_error"
            elif route == "STOCK_ORDER":
                return "process_stock_order"
            else:
                return "process_stock_general"

        # ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(AdvisorState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("classify_main", classify_main_node)
        workflow.add_node("classify_stock", classify_stock_node)
        workflow.add_node("process_stock_order", process_stock_order_node)
        workflow.add_node("process_stock_general", process_stock_general_node)
        workflow.add_node("process_general", process_general_node)
        workflow.add_node("handle_error", handle_error_node)

        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge(START, "classify_main")
        
        # ë©”ì¸ ë¶„ë¥˜ í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "classify_main",
            route_after_main_classification,
            {
                "classify_stock": "classify_stock",
                "process_general": "process_general",
                "handle_error": "handle_error"
            }
        )

        # ì£¼ì‹ ë¶„ë¥˜ í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "classify_stock",
            route_after_stock_classification,
            {
                "process_stock_order": "process_stock_order",
                "process_stock_general": "process_stock_general",
                "handle_error": "handle_error"
            }
        )

        # ëª¨ë“  ì²˜ë¦¬ ë…¸ë“œì—ì„œ ENDë¡œ
        workflow.add_edge("process_stock_order", END)
        workflow.add_edge("process_stock_general", END)
        workflow.add_edge("process_general", END)
        workflow.add_edge("handle_error", END)

        return workflow.compile()

    async def advisor_stream(self, question):
        """ìƒë‹´ ìŠ¤íŠ¸ë¦¼ (í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „)"""
        try:
            # ì‹œì‘ ë©”ì‹œì§€
            classification_data = {"content": f"ğŸ¤– AI ìƒë‹´ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n\n"}
            yield f"data: {json.dumps(classification_data, ensure_ascii=False)}\n\n"

            # LangGraph ì‹¤í–‰
            graph = self._create_langgraph_chain()
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            async for chunk in graph.astream(
                {
                    "question": question,
                    "metadata": {"session_id": "stream", "timestamp": json.dumps({}, default=str)},
                    "processed_data": {}
                }, 
                config={"callbacks": callbacks}
            ):
                # ì¤‘ê°„ ë‹¨ê³„ ì •ë³´ë„ ìŠ¤íŠ¸ë¦¬ë° (ì„ íƒì )
                for node_name, node_output in chunk.items():
                    # ë¶„ë¥˜ ë‹¨ê³„ í”¼ë“œë°±
                    if node_name == "classify_main":
                        route = node_output.get("route", "")
                        if route == "STOCK":
                            feedback = {"content": "ğŸ“ˆ ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤...\n"}
                            yield f"data: {json.dumps(feedback, ensure_ascii=False)}\n\n"
                        elif route == "GENERAL":
                            feedback = {"content": "ğŸ’¬ ì¼ë°˜ ìƒë‹´ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤...\n"}
                            yield f"data: {json.dumps(feedback, ensure_ascii=False)}\n\n"
                    
                    # ìµœì¢… ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                    elif node_name in ["process_stock_order", "process_stock_general", "process_general", "handle_error"]:
                        final_result = node_output.get("final_result")
                        if final_result:
                            content = self._extract_content_for_streaming(final_result)
                            
                            # íƒ€ì…ë³„ ì´ëª¨ì§€ ì¶”ê°€
                            type_emojis = {
                                "stock_advice": "ğŸ“Š ",
                                "general_advice": "ğŸ’¡ ",
                                "order_confirmation": "âœ… ",
                                "error": "âŒ "
                            }
                            
                            result_type = final_result.get("type", "")
                            prefix = type_emojis.get(result_type, "")
                            
                            # ë¬¸ìë³„ ìŠ¤íŠ¸ë¦¬ë°
                            full_content = prefix + content
                            for char in full_content:
                                yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            error_data = {"content": f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
    
    def _extract_content_for_streaming(self, result):
        """ìŠ¤íŠ¸ë¦¬ë°ìš© ì»¨í…ì¸  ì¶”ì¶œ"""
        if isinstance(result, dict):
            content = result.get("content", "")
            if isinstance(content, dict):
                return json.dumps(content, ensure_ascii=False, indent=2)
            return str(content)
        return str(result)

    # ë™ê¸°ì‹ ì²˜ë¦¬ ë©”ì†Œë“œ (í…ŒìŠ¤íŠ¸ìš©)
    def process_question(self, question: str) -> dict:
        """ë™ê¸°ì‹ ì§ˆë¬¸ ì²˜ë¦¬"""
        graph = self._create_langgraph_chain()
        result = graph.invoke({"question": question})
        return result

    # ë””ë²„ê¹…ìš© ë©”ì†Œë“œ
    def debug_classification(self, question: str) -> dict:
        """ë¶„ë¥˜ ê³¼ì • ë””ë²„ê¹…"""
        main_result = self.main_classifier_chain.invoke({"question": question})
        
        debug_info = {
            "question": question,
            "main_classification": main_result,
        }
        
        if main_result.get("is_stock"):
            stock_result = self.stock_classifier_chain.invoke({"question": question})
            debug_info["stock_classification"] = stock_result
        
        return debug_info